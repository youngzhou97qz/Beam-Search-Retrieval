# 模型
# bert 预训练 + transformer + generative
import torch
from transformers import *

pretrained_weights = 'bert-base-chinese'
tokenizer = BertTokenizer.from_pretrained(pretrained_weights)      # tokenizer.vocab_size = 21128
# input_ids = torch.tensor([tokenizer.encode("我是谁？")])
# all_hidden_states, _ = model(input_ids)[-2:]
# print(all_hidden_states.shape)

def gelu(x):
    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))

class Pre_trained(nn.Module):
    def __init__(self, model=BertModel.from_pretrained(pretrained_weights)):
        super().__init__()
        self.model = model
        for p in self.parameters():
            p.requires_grad=False
    def forward(self, input_ids):
        input_ids = torch.tensor(input_ids).to(device)
        hidden_states, _ = model(input_ids)[-2:]
        return hidden_states

class MultiHeadedSelfAttention(nn.Module):
    def __init__(self):
        super().__init__()
        self.proj_q = nn.Linear(dim, dim)
        self.proj_k = nn.Linear(dim, dim)
        self.proj_v = nn.Linear(dim, dim)
        self.drop = nn.Dropout(drop)
        self.scores = None
        self.n_heads = heads
    def forward(self, x, mask):
        q, k, v = self.proj_q(x), self.proj_k(x), self.proj_v(x)
        q, k, v = (self.split_last(x, (self.n_heads, -1)).transpose(1, 2) for x in [q, k, v])
        scores = q @ k.transpose(-2, -1) / np.sqrt(k.size(-1))
        if mask is not None:
            mask = mask[:, None, None, :].float()
            scores -= 10000.0 * (1.0 - mask)
        scores = self.drop(F.softmax(scores, dim=-1))
        h = (scores @ v).transpose(1, 2).contiguous()
        h = self.merge_last(h, 2)
        self.scores = scores
        return h
    def split_last(self, x, shape):
        shape = list(shape)
        assert shape.count(-1) <= 1
        if -1 in shape:
            shape[shape.index(-1)] = int(x.size(-1) / -np.prod(shape))
        return x.view(*x.size()[:-1], *shape)
    def merge_last(self, x, n_dims):
        s = x.size()
        assert n_dims > 1 and n_dims < len(s)
        return x.view(*s[:-n_dims], -1)

class PositionWiseFeedForward(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(dim, dim*ffn)
        self.fc2 = nn.Linear(dim*ffn, dim)
    def forward(self, x):
        return self.fc2(gelu(self.fc1(x)))

class BertLayer(nn.Module):
    def __init__(self, share='none', norm='pre'):
        super(BertLayer, self).__init__()
        self.share = share
        self.norm_pos = norm
        self.norm1 = nn.LayerNorm(dim, eps=eps)
        self.norm2 = nn.LayerNorm(dim, eps=eps)
        self.drop1 = nn.Dropout(drop)
        self.drop2 = nn.Dropout(drop)
        if self.share == 'ffn':
            self.attention = nn.ModuleList([MultiHeadedSelfAttention() for _ in range(n_layers)])
            self.proj = nn.ModuleList([nn.Linear(dim, dim) for _ in range(n_layers)])
            self.feedforward = PositionWiseFeedForward()
        elif self.share == 'att':
            self.attention = MultiHeadedSelfAttention()
            self.proj = nn.Linear(dim, dim)
            self.feedforward = nn.ModuleList([PositionWiseFeedForward() for _ in range(n_layers)])
        elif self.share == 'all':
            self.attention = MultiHeadedSelfAttention()
            self.proj = nn.Linear(dim, dim)
            self.feedforward = PositionWiseFeedForward()
        elif self.share == 'none':
            self.attention = nn.ModuleList([MultiHeadedSelfAttention() for _ in range(n_layers)])
            self.proj = nn.ModuleList([nn.Linear(dim, dim) for _ in range(n_layers)])
            self.feedforward = nn.ModuleList([PositionWiseFeedForward() for _ in range(n_layers)])
    def forward(self, hidden_states, attention_mask, layer_num):
        attention_mask = torch.cuda.LongTensor(attention_mask)
        if self.norm_pos == 'pre':
            if isinstance(self.attention, nn.ModuleList):
                h = self.proj[layer_num](self.attention[layer_num](self.norm1(hidden_states), attention_mask))
            else:
                h = self.proj(self.attention(self.norm1(hidden_states), attention_mask))
            out = hidden_states + self.drop1(h)
            if isinstance(self.feedforward, nn.ModuleList):
                h = self.feedforward[layer_num](self.norm1(out))
            else:
                h = self.feedforward(self.norm1(out))
            out = out + self.drop2(h)
        if self.norm_pos == 'post':
            if isinstance(self.attention, nn.ModuleList):
                h = self.proj[layer_num](self.attention[layer_num](hidden_states, attention_mask))
            else:
                h = self.proj(self.attention(hidden_states, attention_mask))
            out = self.norm1(hidden_states + self.drop1(h))
            if isinstance(self.feedforward, nn.ModuleList):
                h = self.feedforward[layer_num](out)
            else:
                h = self.feedforward(out)
            out = self.norm2(out + self.drop2(h))
        return out

class Final_model(nn.Module):
    def __init__(self):
        super().__init__()
        self.pre_trained = Pre_trained()
        self.n_layers = n_layers
        self.blocks = BertLayer(share='none', norm='pre')
        
        
        self.fc2 = nn.Linear(dim, dim)
        self.norm = nn.LayerNorm(dim, eps=eps)
        embed_weight = self.transformer.embed.word_embeddings.weight
        n_vocab, embed_dim = embed_weight.size()
        self.decoder = nn.Linear(dim, embed_dim, bias=False)
        self.decoder_2 = nn.Linear(embed_dim, n_vocab, bias=False)
        self.decoder_2.weight = embed_weight
        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))
    def forward(self, input_ids, input_mask, masked_pos):
        h = self.pre_trained(input_ids)
        for i in range(self.n_layers):
            h = self.blocks(h, input_mask, i)
        masked_pos = torch.LongTensor(masked_pos)[:, :, None].expand(-1, -1, h.size(-1)).to(device)
        h_masked = torch.gather(h, 1, masked_pos)
        h_masked = self.norm(gelu(self.fc2(h_masked)))
        logits_lm = self.decoder_2(self.decoder(h_masked)) + self.decoder_bias
        return logits_lm
200219

import random
import copy

batch_size = 2
max_len = 3
vocab = 10
miu = 2
epoch = 9999 # 从0开始

train_data = torch.LongTensor([[1,2,9,3,4,9,0,0],[5,3,1,9,6,7,8,9]])
train_seg  = [[0,0,0,1,1,1,0,0],[0,0,0,0,1,1,1,1]]
train_pad  = [[1,1,1,1,1,1,0,0],[1,1,1,1,1,1,1,1]]
train_mask = torch.LongTensor([[1,1,1,0,0,0,0,0],[1,1,1,1,0,0,0,0]])

train_pos  = [3,4]
train_tok  = torch.LongTensor([3,6])
train_enc = train_data * train_mask # torch.Size([2, 8])
train_mode = nn.Embedding(vocab, vocab)(train_enc) # torch.Size([2, 8, 10])

train_pred = torch.randn(2,10)
print(train_pred.shape) # torch.Size([2, 10])

def train(model, iterator, optimizer, epoch, miu=2): #词汇级
    samp = miu/(miu-1+exp(epoch/miu))
    model.train()
    epoch_loss, count = 0, 0
    iter_bar = tqdm(iterator, desc='Training')
    for _, batch in enumerate(iter_bar):
        train_batch = copy.deepcopy(a)
        for t in range(max_len):
            optimizer.zero_grad()
            output = model(batch)
            loss = nn.CrossEntropyLoss(reduction='none')(output, train_tok)
            loss.backward()
            if clip:
                nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            count += 1
            epoch_loss += loss.mean().item()
            iter_bar.set_description('loss=%5.3f'%loss.mean().item())
            
            if samp > 1/(miu+1):
                if random.random() < samp:
                    取原始结果
                else:
                    取beam结果
            else:
                if random.random() < samp:
                    取原始结果
                elif random.random() < miu * samp
                    取beam结果
                else:
                    取生成结果
            返回 train_batch
    return epoch_loss / count

def valid(): #句子级
