# 模型
# bert 预训练 + transformer + generative
import numpy as np
import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import *
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
pretrained_weights = 'bert-base-chinese'
tokenizer = BertTokenizer.from_pretrained(pretrained_weights)      # tokenizer.vocab_size = 21128

def gelu(x):
    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))

class Pre_trained(nn.Module):
    def __init__(self, model=BertModel.from_pretrained(pretrained_weights)):
        super().__init__()
        self.model = model
        for p in self.parameters():
            p.requires_grad=False
    def forward(self, input_ids, segment_ids):
        input_ids = input_ids.clone().detach().to(device)
        segment_ids = segment_ids.clone().detach().to(device)
        self.model.eval()
        with torch.no_grad():
            hidden_states, _ = self.model(input_ids, token_type_ids=segment_ids)
        return hidden_states

class MultiHeadedSelfAttention(nn.Module):
    def __init__(self, dim=768, drop=0.1, heads=12):
        super().__init__()
        self.proj_q = nn.Linear(dim, dim)
        self.proj_k = nn.Linear(dim, dim)
        self.proj_v = nn.Linear(dim, dim)
        self.drop = nn.Dropout(drop)
        self.scores = None
        self.n_heads = heads
    def forward(self, x, mask):
        q, k, v = self.proj_q(x), self.proj_k(x), self.proj_v(x)
        q, k, v = (self.split_last(x, (self.n_heads, -1)).transpose(1, 2) for x in [q, k, v])
        scores = q @ k.transpose(-2, -1) / np.sqrt(k.size(-1))
        if mask is not None:
            mask = mask[:, None, None, :].float()
            scores -= 10000.0 * (1.0 - mask)
        scores = self.drop(F.softmax(scores, dim=-1))
        h = (scores @ v).transpose(1, 2).contiguous()
        h = self.merge_last(h, 2)
        self.scores = scores
        return h
    def split_last(self, x, shape):
        shape = list(shape)
        assert shape.count(-1) <= 1
        if -1 in shape:
            shape[shape.index(-1)] = int(x.size(-1) / -np.prod(shape))
        return x.view(*x.size()[:-1], *shape)
    def merge_last(self, x, n_dims):
        s = x.size()
        assert n_dims > 1 and n_dims < len(s)
        return x.view(*s[:-n_dims], -1)

class PositionWiseFeedForward(nn.Module):
    def __init__(self, dim=768, ffn=4):
        super().__init__()
        self.fc1 = nn.Linear(dim, dim*ffn)
        self.fc2 = nn.Linear(dim*ffn, dim)
    def forward(self, x):
        return self.fc2(gelu(self.fc1(x)))

class BertLayer(nn.Module):
    def __init__(self, share='none', norm='pre', dim=768, eps=1e-12, drop=0.1, n_layers=4):
        super(BertLayer, self).__init__()
        self.share = share
        self.norm_pos = norm
        self.norm1 = nn.LayerNorm(dim, eps=eps)
        self.norm2 = nn.LayerNorm(dim, eps=eps)
        self.drop1 = nn.Dropout(drop)
        self.drop2 = nn.Dropout(drop)
        if self.share == 'ffn':
            self.attention = nn.ModuleList([MultiHeadedSelfAttention() for _ in range(n_layers)])
            self.proj = nn.ModuleList([nn.Linear(dim, dim) for _ in range(n_layers)])
            self.feedforward = PositionWiseFeedForward()
        elif self.share == 'att':
            self.attention = MultiHeadedSelfAttention()
            self.proj = nn.Linear(dim, dim)
            self.feedforward = nn.ModuleList([PositionWiseFeedForward() for _ in range(n_layers)])
        elif self.share == 'all':
            self.attention = MultiHeadedSelfAttention()
            self.proj = nn.Linear(dim, dim)
            self.feedforward = PositionWiseFeedForward()
        elif self.share == 'none':
            self.attention = nn.ModuleList([MultiHeadedSelfAttention() for _ in range(n_layers)])
            self.proj = nn.ModuleList([nn.Linear(dim, dim) for _ in range(n_layers)])
            self.feedforward = nn.ModuleList([PositionWiseFeedForward() for _ in range(n_layers)])
    def forward(self, hidden_states, attention_mask, layer_num):
        attention_mask = torch.cuda.LongTensor(attention_mask)
        if self.norm_pos == 'pre':
            if isinstance(self.attention, nn.ModuleList):
                h = self.proj[layer_num](self.attention[layer_num](self.norm1(hidden_states), attention_mask))
            else:
                h = self.proj(self.attention(self.norm1(hidden_states), attention_mask))
            out = hidden_states + self.drop1(h)
            if isinstance(self.feedforward, nn.ModuleList):
                h = self.feedforward[layer_num](self.norm1(out))
            else:
                h = self.feedforward(self.norm1(out))
            out = out + self.drop2(h)
        if self.norm_pos == 'post':
            if isinstance(self.attention, nn.ModuleList):
                h = self.proj[layer_num](self.attention[layer_num](hidden_states, attention_mask))
            else:
                h = self.proj(self.attention(hidden_states, attention_mask))
            out = self.norm1(hidden_states + self.drop1(h))
            if isinstance(self.feedforward, nn.ModuleList):
                h = self.feedforward[layer_num](out)
            else:
                h = self.feedforward(out)
            out = self.norm2(out + self.drop2(h))
        return out

class Final_model(nn.Module):
    def __init__(self, n_layers=4, dim=768, eps=1e-12, n_vocab=2500):
        super().__init__()
        self.pre_trained = Pre_trained()
        self.n_layers = n_layers
        self.blocks = BertLayer()
        self.fc2 = nn.Linear(dim, dim)
        self.norm = nn.LayerNorm(dim, eps=eps)
        self.decoder = nn.Linear(dim, n_vocab)
    def forward(self, input_ids, segment_ids, input_mask, masked_pos):
        h = self.pre_trained(input_ids, segment_ids)
        for i in range(self.n_layers):
            h = self.blocks(h, input_mask, i)
        masked_pos = torch.LongTensor(masked_pos)[:, :, None].expand(-1, -1, h.size(-1)).to(device)
        h_masked = torch.gather(h, 1, masked_pos)
        h_masked = self.decoder(self.norm(gelu(self.fc2(h_masked))))
        return h_masked

# model_a = Final_model().to(device)
# a = torch.tensor([[2769, 3221, 6443, 8043],[2769, 3221, 6443, 8043]]).to(device)
# b = torch.tensor([[1, 1, 1, 0],[1, 1, 1, 0]]).to(device)
# c = ([[2],[3]])
# e = model_a(a,b,b,c)
# print(e.shape)

# def get_parameter_number(net):
#     total_num = sum(p.numel() for p in net.parameters())
#     trainable_num = sum(p.numel() for p in net.parameters() if p.requires_grad)
#     return {'Total': total_num, 'Trainable': trainable_num}
# print(get_parameter_number(MultiHeadedSelfAttention()))
# print(get_parameter_number(PositionWiseFeedForward()))
# print(get_parameter_number(BertLayer()))
# print(get_parameter_number(Final_model()))

# 数据
# read data
questions, answers = [], []
f = open('/home/dango/STC3/data/questions.txt','r',encoding='gbk')
lines = f.readlines()
for line in lines:
    line = line.strip()
    questions.append(line)
f.close()
f = open('/home/dango/STC3/data/answers.txt','r',encoding='gbk')
lines = f.readlines()
for line in lines:
    line = line.strip()
    answers.append(line)
f.close()

# judging chinese
def check_contain_chinese(check_str):
    for ch in check_str:
        if u'\u4e00' <= ch <= u'\u9fff':
            return True
    return False
# delete non-Chinese
i = 0
while 1:
    j = len(questions)
    if i >= j:
        break
    if check_contain_chinese(questions[i]) and check_contain_chinese(answers[i]):
        i += 1
    else:
        questions.pop(i)
        answers.pop(i)

# standardization
from itertools import groupby
def pre_process(text):
    for j in range(len(text)):
        # expression
        text[j] = text[j].replace('[ [', '[').replace('] ]', ']')
        zuo, you = 0, 0
        g = len(text[j])-1
        while 1:
            if text[j][g] ==  ']':
                you = g
            elif text[j][g] == '[':
                zuo = g
                for h in range(you,zuo,-1):
                    if text[j][h] == ' ':
                        you -= 1
                        text[j] = text[j][:h] + text[j][h+1:]
            if g == 0:
                break
            else:
                g -= 1
        # duplicate words
        text[j] = text[j].split(' ')
        text[j] = [x[0] for x in groupby(text[j])]
        for index, string in enumerate(text[j]):
            if len(string) > 3:
                char_list = list(string)
                for i in range(len(string)-3):
                    if char_list[len(string)-1-i] == char_list[len(string)-2-i] and char_list[len(string)-1-i] == char_list[len(string)-3-i] and char_list[len(string)-1-i] == char_list[len(string)-4-i]:
                        char_list.pop(len(string)-1-i)
                text[j][index] = "".join(char_list)
        g = 0
        while 1:
            if text[j][g] == '':
                text[j].pop(g)
            else:
                g += 1
            if g > len(text[j])-1:
                break
        temp = ''.join(text[j])
        text[j] = list(temp)
    return text
questions = pre_process(questions)
answers = pre_process(answers)

ques = questions[:4]
answ = answers[:4]
for i in range(4):
    ques[i] = ''.join(ques[i])
    answ[i] = ''.join(answ[i])
# print(ques)

# 答句2500词
# import collections
# # initial dictionary
# char_answ = []
# for i in range(len(answers)):
#     for j in range(len(answers[i])):
#         char_answ.append(answers[i][j])
# answ_dict = collections.Counter(char_answ)
# word frequency
# rest_answ = dict(filter(lambda x: (x[1] > 250 and (x[0] >= '\u4e00' and x[0] <= '\u9fff')) or (x[1] > 500 and (x[0] < '\u4e00' or x[0] > '\u9fff')), answ_dict.items()))
# print('答词典数：',len(rest_answ))
# count = 1
# for key in rest_answ.keys():
#     rest_answ[key] = count
#     count += 1
# rest_answ['[SEP]'] = 0

tokenizer=BertTokenizer.from_pretrained(pretrained_weights)
token_id = tokenizer.convert_tokens_to_ids

def gene_loader(ques, answ, batch_size, max_len):
    count = 0
    while count < len(ques):
        batch = []
        size = min(batch_size, len(ques) - count)
        for _ in range(size):
#             part1 = tokenizer.encode(random_characters(ques[count])) #改用BERT替换问句词
            part1 = tokenizer.encode(ques[count])
            part2 = tokenizer.encode(answ[count])
            truncate_tokens(part1, part2, max_len-2)
            tokens = part1 + token_id(['[SEP]']) + part2 + token_id(['[SEP]'])
            temp_tokens = part1 + token_id(['[SEP]'])
            segment_ids = [0]*(len(part1)+1) + [1]
            num = len(part1)+1
            input_mask = [1]*(num+1)
            masked_tokens, masked_pos = [], []
            masked_tokens.append(tokens[num])
            masked_pos.append(num)
            n_pad = max_len - num - 1
            tokens.extend([0]*(max_len - len(tokens)))
            temp_tokens.extend([0]*(max_len - len(temp_tokens)))
            segment_ids.extend([0]*n_pad)
            input_mask.extend([0]*n_pad) 
            batch.append((tokens, temp_tokens, segment_ids, input_mask, masked_pos, masked_tokens))
            count += 1
        yield batch

def truncate_tokens(tokens_a, tokens_b, max_len):
    while True:
        if len(tokens_a) + len(tokens_b) <= max_len:
            break
        if len(tokens_a) > len(tokens_b):
            tokens_a.pop()
        else:
            tokens_b.pop()

# def random_characters(token, proportion=0.14):  # 调参
#     token = list(token)
#     if len(token) > 7:
#         num = min(int(len(token)*proportion), 4)
#         for _ in range(num):
#             index = random.randint(0,len(token)-1)
#             token[index] = random.choice(list(rest_answ))
#     return ''.join(token)

a = gene_loader(ques, answ, 2, 64)
print(next(a))

a = gene_loader(ques, answ, 2, 64)
tokens, temp_tokens, segment_ids, input_mask, masked_pos, masked_tokens = next(a)
print(len(tokens),len(temp_tokens),len(segment_ids),len(input_mask),len(masked_pos),len(masked_tokens))

# 替换问句词
# print(ques)
# '哈哈、生日快乐。我地居然同一日生日'
text = ques[1]
tokenized_text = tokenizer.tokenize(text)

# Mask a token that we will try to predict back with `BertForMaskedLM`
masked_index = 9
tokenized_text[masked_index] = '[MASK]'
print(tokenized_text)

# Convert token to vocabulary indices
indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)
# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)
segments_ids = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

# Convert inputs to PyTorch tensors
tokens_tensor = torch.tensor([indexed_tokens])
segments_tensors = torch.tensor([segments_ids])

# Load pre-trained model (weights)
model = BertForMaskedLM.from_pretrained('bert-base-chinese')
model.eval()

# If you have a GPU, put everything on cuda
tokens_tensor = tokens_tensor.to('cuda')
segments_tensors = segments_tensors.to('cuda')
model.to('cuda')

# Predict all tokens
with torch.no_grad():
    outputs = model(tokens_tensor, token_type_ids=segments_tensors)
    predictions = outputs[0]

# confirm we were able to predict 'henson'
predicted_index = torch.argmax(predictions[0, masked_index]).item()
predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]
print(predicted_token)
